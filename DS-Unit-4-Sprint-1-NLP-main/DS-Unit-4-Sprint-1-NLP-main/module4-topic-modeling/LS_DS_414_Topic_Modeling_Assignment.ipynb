{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Topic Modeling\n",
    "## *Data Science Unit 4 Sprint 1 Assignment 4*\n",
    "\n",
    "**Analyze a corpus of Amazon reviews from Unit 4 Sprint 1 Module 1's using topic modeling: \n",
    "\n",
    "- Load in the Amazon Review dataset\n",
    "- Clean the dataset \n",
    "- Vectorize the dataset \n",
    "- Fit a Gensim LDA topic model on Amazon Reviews\n",
    "- Select appropriate number of topics\n",
    "- Create some dope visualization of the topics\n",
    "- Write a few bullets on your findings in markdown at the end\n",
    "- **Note**: You don't *have* to use generators for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Load the Amazon Review corpus \n",
    "This dataset is located in the Sprint 1 Module 1 directory. \n",
    "\n",
    "If the provided relative path doesn't work for you, then you'll have to provide the file path so pandas can read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../sprint-1-module-1-text-data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Clean data\n",
    "\n",
    "- Create a function called `clean_data` that uses regex expressions to clean your data in preparation for the vectorizer. \n",
    "\n",
    "- Save the clean text data to a column in your dataframe named `clean_text`\n",
    "\n",
    "- Feel free to re-use old code that you have written in previous modules  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2513c0ebd04cf8645fe0a1feb1e1a36",
     "grade": false,
     "grade_id": "cell-fa0950cfe5ef7725",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6ceefe5476e71efcb3d36c3e203616f",
     "grade": false,
     "grade_id": "cell-5b8e2bc0f9a745f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# save to cleaned review articles to a feature named `clean_text`\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphebetical_chars = [\"ABCDEFGHIJKLMNOP\"]\n",
    "# check if any of these alphabetical chars exist in your clean chars\n",
    "assert df.clean_text.isin(alphebetical_chars).sum() == 0, \"Did you case normalize your text inside of your clean_data function?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Determine number of topics\n",
    "\n",
    "We are going to run an experiment to determine how many topics exists within the `primaryCategories` of `Electronics`. This is the largest primary category containing nearly 14K documents, so we should have plenty of data. \n",
    "\n",
    "Just as we did in the guided project, we'll be running a gridseach over the number of topics and scoring each model using the Coherence metric to determine which number of topics we should use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask for docs that are in the Electronics primaryCategories - save result to `electronics_mask`\n",
    "electronics_mask = df.primaryCategories.isin([\"Electronics\"])\n",
    "\n",
    "# use msak to isolate all the documents in the Electronics primaryCategories - save result to `df_electronics`\n",
    "df_electronics = df[electronics_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_electronics.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Tokenize your documents \n",
    "\n",
    "Remember that you'll need to use the [**corpora**](https://radimrehurek.com/gensim/corpora/dictionary.html) class from the Gensim library. So definitely check out the docs to learn more about this tool. There is an example on how to do this in the guided project.\n",
    "\n",
    "But before we can use the [**corpora**](https://radimrehurek.com/gensim/corpora/dictionary.html) class, we must first tokenize our articles. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f9f136b6fa56339c87e209fe6f2f12d",
     "grade": false,
     "grade_id": "cell-d7d585b64d6f0119",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# identify how many processors your machine has - save the result to `n_processors`\n",
    "\n",
    "# subtract 1 from n_processors - save the result to `nb_workers`\n",
    "\n",
    "# initialize just like we did in the guided project\n",
    "# COLAB only has 2 processors, set nb_workers=2 and hope that it doesn't crash your notebook (otherwise don't use this tool)\n",
    "#pandarallel.initialize(progress_bar=True, nb_workers=nb_workers)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the spaCy language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create our tokens in the form of lemmas \n",
    "df_electronics['lemmas'] = df_electronics['clean_text'].parallel_apply(lambda x: [token.lemma_ for token in nlp(x) if (token.is_stop != True) and (token.is_punct != True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the corpora class to prep your data for LDA\n",
    "\n",
    "You'll need to create the same `id2word` and `corpus` objects that we created in the guided projects. So be sure to reference the guided project notebook if you need to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53bf811aebb39e7ed6fc592851f82f8b",
     "grade": false,
     "grade_id": "cell-9d92f28649aa999e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create lemma dictionary using Dictionary - save result to `id2word`\n",
    "\n",
    "# Create Term Document Frequency list - save result to `corpus`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch the number of topics \n",
    "\n",
    "Just as we did in the guided project, we're going to run a for loop over a range of possible number of topics and then plot the coherence values to determine which number of topics leads to the most sensible grouping of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                        id2word=id2word,\n",
    "                                                        num_topics=num_topics, \n",
    "                                                        chunksize=100,\n",
    "                                                        passes=10,\n",
    "                                                        random_state=1234,\n",
    "                                                        per_word_topics=True,\n",
    "                                                        workers=10)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=df_electronics['lemmas'], start=2, limit=22, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=2; limit=22;  step=2;\n",
    "x = range(start, limit, step)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.grid()\n",
    "plt.title(\"Coherence Score vs. Number of Topics\")\n",
    "plt.xticks(x)\n",
    "plt.plot(x, coherence_values, \"-o\")\n",
    "\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45f72ff929ab8b765d0f0b5f35137f0d",
     "grade": false,
     "grade_id": "cell-e97661faebf1ac3c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use np.argmax() to get index of largest coherence value from coherence_values - save result to `max_cohereance_val_index`\n",
    "\n",
    "# use `max_cohereance_val_index` to index model_list for the corresponding model - save result to `lda_trained_model`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pyLDAvis to visual your topics \n",
    "\n",
    "Take a look at the topic bubbles and bar char for the terms on the right hand side.  \n",
    "\n",
    "- Describe the topic bubbles. \n",
    "- Do they overlap or not? \n",
    "- What does it mean when they overlap? \n",
    "- What does it mean when they don't overlap?\n",
    "- Are the terms in each topic distinct from the topics in the other topic bubbles?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ad72f47a2b4930f8503c7733e5257cc",
     "grade": false,
     "grade_id": "cell-5213c8c2af79e714",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot your topics here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Topic id/name dictionary \n",
    "\n",
    "When populating your topic id/name dictionary, use the index ordering as shown in the viz tool. \n",
    "\n",
    "We'll use a function to map the the viz tool index ordering with the train LDA model ordering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc10e9728e5dcf06baf0800c3cdb88ce",
     "grade": false,
     "grade_id": "cell-4905c0c1050f0d03",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create a dictionary \n",
    "# keys - use topic ids from pyLDAvis visualization \n",
    "# values - topic names that you create \n",
    "# save dictionary to `vis_topic_name_dict`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_id_lookup_dict(vis, vis_topic_name_dict):\n",
    "    \"\"\"\n",
    "    Both the starting index and the ordering of topic ids bewteen the trained LDA model \n",
    "    and the viz tool are different. So we need to create a look up dictionary that maps \n",
    "    the correct association between topic ids from both sources. \n",
    "    \"\"\"\n",
    "    # value is order of topic ids accoridng to pyLDAvis tool \n",
    "    # key is order of topic ids according to lda model\n",
    "    model_vis_tool_topic_id_lookup = vis.topic_coordinates.topics.to_dict()\n",
    "\n",
    "    # invert dictionary so that \n",
    "    # key is order of topic ids accoridng to pyLDAvis tool \n",
    "    # value is order of topic ids according to lda model\n",
    "    topic_id_lookup =  {v:k for k, v in model_vis_tool_topic_id_lookup.items()}\n",
    "    \n",
    "    return {v:vis_topic_name_dict[k]  for k, v in topic_id_lookup.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55f23aa9324a225091037f0c5daf2192",
     "grade": false,
     "grade_id": "cell-d38acb7b250b4079",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Each Document a Topic Name\n",
    "\n",
    "Now that we have a topic id/name look up dict that is aligned with the index ordering of the trained LDA model, we can move forward to giving each topic a topic name. \n",
    "\n",
    "The function below has been given to you. However, you highly encouraged to read through it and make sure that you understand what it is doing each step of the way. In fact, a good way to do this is to copy and paste the code inside of the function into a new cell, comment out all the lines of code and line by line, uncomment the code and see the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_ids_for_docs(lda_model, corpus):\n",
    "    \n",
    "    \"\"\"\n",
    "    Passes a Bag-of-Words vector into a trained LDA model in order to get the topic id of that document. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model: Gensim object\n",
    "        Must be a trained model \n",
    "        \n",
    "    corpus: nested lists of tuples, \n",
    "        i.e. [[(),(), ..., ()], [(),(), ..., ()], ..., [(),(), ..., ()]]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    topic_id_list: list\n",
    "        Contains topic ids for all document vectors in corpus \n",
    "    \"\"\"\n",
    "    \n",
    "    # store topic ids for each document\n",
    "    doc_topic_ids = []\n",
    "\n",
    "    # iterature through the bow vectors for each doc\n",
    "    for doc_bow in corpus:\n",
    "        \n",
    "        # store the topic ids for the doc\n",
    "        topic_ids = []\n",
    "        # store the topic probabilities for the doc\n",
    "        topic_probs = []\n",
    "\n",
    "        # list of tuples\n",
    "        # each tuple has a topic id and the prob that the doc belongs to that topic \n",
    "        topic_id_prob_tuples = lda_trained_model.get_document_topics(doc_bow)\n",
    "        \n",
    "        # iterate through the topic id/prob pairs \n",
    "        for topic_id_prob in topic_id_prob_tuples:\n",
    "            \n",
    "            # index for topic id\n",
    "            topic_id = topic_id_prob[0]\n",
    "            # index for prob that doc belongs that the corresponding topic\n",
    "            topic_prob = topic_id_prob[1]\n",
    "\n",
    "            # store all topic ids for doc\n",
    "            topic_ids.append(topic_id)\n",
    "            # store all topic probs for doc\n",
    "            topic_probs.append(topic_prob)\n",
    "\n",
    "        # get index for largest prob score\n",
    "        max_topic_prob_ind = np.argmax(topic_probs)\n",
    "        # get corresponding topic id\n",
    "        max_prob_topic_id = topic_ids[max_topic_prob_ind]\n",
    "        # store topic id that had the highest prob for doc being a memebr of that topic\n",
    "        doc_topic_ids.append(max_prob_topic_id)\n",
    "        \n",
    "    return doc_topic_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09f2b8ebaf696dc05187cecce8c9e251",
     "grade": false,
     "grade_id": "cell-e28e8774a79ac647",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# use get_topic_ids_for_docs get the topic id for each doc in the corpus - save result to `doc_topic_ids`\n",
    "\n",
    "# create a new feature in df_electronics called topic_id using `doc_topic_ids`\n",
    "\n",
    "# iterate through topic_id and use the lookup dict `topic_name_dict` to assign each document a topic name\n",
    "# save results to a new feature in df_electronics called `new_topic_name`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have created new topic names for your documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"reviews.text\", \"new_topic_name\"]\n",
    "df_electronics[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Stretch Goals\n",
    "\n",
    "- Treat `new_topic_name` as a Y vector and train a supervised learning model to predict the topic of each document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37  (Python3)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
